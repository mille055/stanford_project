{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mille055/stanford_project/blob/dockerversion/app/notebooks/Stanford_CS231N.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LRESl50fER75"
      },
      "outputs": [],
      "source": [
        "COLAB_FLAG = True   # whether running on colab or locally on computer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAb0TJdUER77",
        "outputId": "79bd2333-7848-42d2-93e8-4558e6404a58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydicom==2.1.2\n",
            "  Downloading pydicom-2.1.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.1.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting monai\n",
            "  Downloading monai-1.1.0-202212191849-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.10/dist-packages (from monai) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from monai) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.5.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.1+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->seaborn) (2022.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->monai) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->monai) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->monai) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8->monai) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8->monai) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.8->monai) (16.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8->monai) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8->monai) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=77acda56aebfbe9b683cb590a9bec8e0ad098214a13ebd785fb375eba449f1d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers, monai\n",
            "Successfully installed huggingface-hub-0.14.1 monai-1.1.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.1\n",
            "Cloning into 'stanford_project'...\n",
            "remote: Enumerating objects: 2273, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 2273 (delta 15), reused 0 (delta 0), pack-reused 2245\u001b[K\n",
            "Receiving objects: 100% (2273/2273), 268.30 MiB | 51.11 MiB/s, done.\n",
            "Resolving deltas: 100% (1615/1615), done.\n"
          ]
        }
      ],
      "source": [
        "if COLAB_FLAG:\n",
        "  %pip install pydicom==2.1.2\n",
        "  %pip install monai seaborn sentence_transformers\n",
        "  !git clone -b dockerversion 'https://github.com/mille055/stanford_project.git'\n",
        "  \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pQECLOZTER78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import pydicom\n",
        "import pickle\n",
        "import glob\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, ConfusionMatrixDisplay\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from pprint import pprint\n",
        "from fastai.basics import delegates\n",
        "from fastcore.parallel import parallel\n",
        "from fastcore.utils import gt\n",
        "from fastcore.foundation import L\n",
        "\n",
        "from pydicom.dataset import Dataset as DcmDataset\n",
        "from pydicom.tag import BaseTag as DcmTag\n",
        "from pydicom.multival import MultiValue as DcmMultiValue\n",
        "import sys\n",
        "import importlib\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z99eSWdLER79",
        "outputId": "68884e2c-6e0d-4e56-f7d4-3f65640f6a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#local imports\n",
        "if COLAB_FLAG:\n",
        "    sys.path.append('/content/stanford_project/app/scripts/')\n",
        "    train_datafile = '/content/stanford_project/app/data/trainfiles.csv'\n",
        "    val_datafile = '/content/stanford_project/app/data/valfiles.csv'\n",
        "    test_datafile = '/content/stanford_project/app/data/testfiles.csv'\n",
        "    \n",
        "    drive.mount('/content/gdrive')\n",
        "\n",
        "else: # running locally\n",
        "    sys.path.append('/Users/cmm/Documents/GitHub/new/stanford_project/app/scripts/')\n",
        "    #sys.path.append('/workspaces/stanford_project/app/scripts/')\n",
        "    train_datafile = '../data/trainfiles.csv'\n",
        "    val_datafile = '../data/valfiles.csv'\n",
        "    test_datafile = '../data/testfiles.csv'\n",
        "\n",
        "\n",
        "### local imports ###\n",
        "from config import file_dict, feats, feats_to_keep, column_lists, RF_parameters, classes, model_paths\n",
        "from config import abd_label_dict, val_list, train_val_split_percent, random_seed, data_transforms\n",
        "from config import sentence_encoder, series_description_column\n",
        "from utils import *\n",
        "\n",
        "# from NLP.NLP_training import train_NLP_model\n",
        "# from NLP.NLP_inference import get_NLP_inference\n",
        "from cnn.cnn_dataset import ImgDataset\n",
        "from cnn.cnn_inference import image_to_tensor, pixel_inference, test_pix_model, load_pixel_model, visualize_results\n",
        "from cnn.cnn_model import *\n",
        "from cnn.cnn_training import *\n",
        "from cnn.cnn_data_loaders import get_data_loaders\n",
        "# from metadata.meta_inference import meta_inference, calc_feature_importances\n",
        "# from metadata.meta_training import train_fit_parameter_trial, train_meta_model, evaluate_meta_model\n",
        "# from fusionmodel.fus_model import FusionModel\n",
        "# from fusionmodel.fus_inference import get_fusion_inference, get_fusion_inference_from_file\n",
        "# from fusionmodel.fus_training import train_fusion_model, CustomDataset\n",
        "from model_container import ModelContainer\n",
        "# from process_tree import Processor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-fMCfj81ER7-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train_df, val_df, test_df = create_datasets(train_datafile, val_datafile, test_datafile, '/volumes/cm7/Abdominal_MRI_dataset_split/', '/content/gdrive/MyDrive/WW_MRI_abd2/split/' )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Different combinations to evaluate\n",
        "models = [CustomResNet50, CustomResNet50b, CustomDenseNet]\n",
        "optimizers = [optim.SGD(model.parameters(), lr=0.001, momentum=0.9), optim.Adam]\n",
        "loss_funcs = [nn.CrossEntropyLoss, FocalLoss]\n",
        "\n"
      ],
      "metadata": {
        "id": "LR9U7i16fhjg",
        "outputId": "d9d697af-2903-4a87-a863-656b45d6732f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-edcfedd1c499>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Different combinations to evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCustomResNet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCustomResNet50b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCustomDenseNet\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mloss_funcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFocalLoss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: SGD.__init__() missing 1 required positional argument: 'params'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_CK2PGiTER7_"
      },
      "outputs": [],
      "source": [
        "# Get the data loaders\n",
        "batch_size = 64\n",
        "train_loader, val_loader, test_loader, dataset_sizes = get_data_loaders(train_df, val_df, test_df, batch_size)\n",
        "dataloaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spQzGoBgER7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14219249-b6a1-4dbd-adef-8bf0d3a84dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with CustomResNet50, SGD, CrossEntropyLoss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 234MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 2.6141 Acc: 0.1841\n",
            "val Loss: 2.3900 Acc: 0.3277\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 1.9564 Acc: 0.4850\n",
            "val Loss: 1.8098 Acc: 0.4887\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 1.4499 Acc: 0.6267\n",
            "val Loss: 1.4298 Acc: 0.6299\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 1.1060 Acc: 0.7180\n",
            "val Loss: 1.1944 Acc: 0.6751\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.8801 Acc: 0.7845\n",
            "val Loss: 1.0392 Acc: 0.7316\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.7112 Acc: 0.8232\n",
            "val Loss: 0.9521 Acc: 0.7232\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.5869 Acc: 0.8612\n",
            "val Loss: 0.8635 Acc: 0.7571\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.5124 Acc: 0.8663\n",
            "val Loss: 0.8545 Acc: 0.7684\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.5104 Acc: 0.8714\n",
            "val Loss: 0.8501 Acc: 0.7655\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.4968 Acc: 0.8766\n",
            "val Loss: 0.8418 Acc: 0.7684\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.4823 Acc: 0.8897\n",
            "val Loss: 0.8461 Acc: 0.7684\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.4766 Acc: 0.8912\n",
            "val Loss: 0.8351 Acc: 0.7712\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.4734 Acc: 0.8853\n",
            "val Loss: 0.8316 Acc: 0.7740\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.4542 Acc: 0.8955\n",
            "val Loss: 0.8272 Acc: 0.7740\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.4595 Acc: 0.8948\n",
            "val Loss: 0.8225 Acc: 0.7768\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n"
          ]
        }
      ],
      "source": [
        "# perform several training runs to compare accuracy on the validation dataset\n",
        "\n",
        "for model_class in models:\n",
        "        for opt_class in optimizers:\n",
        "            for loss_func_class in loss_funcs:\n",
        "                print(f\"\\nTraining with {model_class.__name__}, {opt_class.__name__}, {loss_func_class.__name__}\")\n",
        "\n",
        "                # Instantiate the custom model\n",
        "                num_classes = len(classes)\n",
        "                model = model_class(num_classes)\n",
        "                model = model.to(device)  # Move the model to the appropriate device\n",
        "\n",
        "                # Define loss function and optimizer\n",
        "                criterion = loss_func_class()\n",
        "                optimizer = opt_class(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "                # Decay LR by a factor of 0.1 every 7 epochs\n",
        "                exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n",
        "                # Train the model\n",
        "                trained_model, best_val_accuracy, history = train_cnn_model(model, dataloaders, criterion, optimizer, exp_lr_scheduler, num_epochs=25)\n",
        "\n",
        "                # Plot training curves\n",
        "                combination_name = f\"{model_class.__name__}_{opt_class.__name__}_{loss_func_class.__name__}\"\n",
        "                plot_training_curves(history, combination_name)\n",
        "\n",
        "                # Evaluate the model on the test dataset\n",
        "                preds, probs = pixel_inference(test.fname.tolist())\n",
        "                true = test.true\n",
        "                accuracy = np.sum(preds==true)/len(true)\n",
        "                print('Accuracy on the test dataset is ', np.round(accuracy, 3))\n",
        "                #results = make_results_df(preds, true, test)\n",
        "\n",
        "                # Save the trained model if needed\n",
        "                save_filename = f\"cnn_model_{combination_name}_{datetime.now().strftime('%Y%m%d')}.pth\"\n",
        "                torch.save(trained_model.state_dict(), save_filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pxbvZqmHhck4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}